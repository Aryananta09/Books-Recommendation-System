# -*- coding: utf-8 -*-
"""Submission Akhir - Sistem Rekomendasi - Muhammad Awwal Aryananta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RnyMzCiISdIqOuHrOqwEhm0dHw-zZ_dR

# Import Library
"""

# Import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""Melakukan import seluruh library yang dibutuhkan, mulai dari mengolah data, visualisasi, prapemrosesan data, hingga pembangunan dan pelatihan model machine learning sistem rekomendasi baik content-based filtering (TF-IDF & Cosine)maupun collaborative filtering

# Data Loading
"""

books = pd.read_csv("Books.csv")
ratings = pd.read_csv("Ratings.csv")
users = pd.read_csv("Users.csv")

"""Melakukan load tiga file utama secara lokal dari dataset rekomendasi buku, yaitu Books.csv (data buku), Ratings.csv (data rating pengguna), dan Users.csv (data pengguna) untuk keperluan analisis dan pembuatan sistem rekomendasi."""

print('Jumlah data buku: ', len(books.ISBN.unique()))
print('Jumlah data penilaian buku: ', len(ratings))
print('Jumlah data pengguna: ', len(users['User-ID'].unique()))

"""Menampilkan jumlah data masing-masing dari buku (berdasarkan ISBN), data penilaian atau rating, dan jumlah pengguna (berdasarkan User-ID) yang terdapat dalam dataset.

# Exploratory Data Analysis (EDA)

## Mengeksplorasi Data Buku
"""

books.info()

"""Menampilkan ringkasan informasi dari data buku mulai dari kolom, jumlah baris, dan tipe data tiap kolomnya. Terlihat ada 8 kolom dengan jumlah baris 271.360 baris data dan semua tipe data kolomnya adalah object."""

print('Banyak data: ', len(books.ISBN.unique()))

print('Total Author unik:', books['Book-Author'].nunique())
print('Beberapa Author:', books['Book-Author'].unique()[:10])

print('Total Tahun Publikasi unik:', books['Year-Of-Publication'].nunique())
print('Beberapa Tahun Publikasi:', books['Year-Of-Publication'].unique()[:10])

print('Total Publisher unik:', books['Publisher'].nunique())
print('Beberapa Publisher:', books['Publisher'].unique()[:10])

"""Menampilkan total jumlah buku berdasarkan ISBN unik, serta menghitung banyaknya penulis, tahun terbit, dan penerbit yang berbeda. Ditampilkan juga beberapa contoh nilai dari tiap kolom untuk memberikan ilustrasi isi data."""

# Top 20 Author dengan jumlah buku terbanyak
top_authors = books['Book-Author'].value_counts().head(20).reset_index()
top_authors.columns = ['Author', 'Jumlah Buku']

plt.figure(figsize=(12, 6))
sns.barplot(data=top_authors, x='Jumlah Buku', y='Author', hue='Author', palette='viridis', legend=False)
plt.title('Top 20 Author dengan Jumlah Buku Terbanyak')
plt.xlabel('Jumlah Buku')
plt.ylabel('Author')
plt.tight_layout()
plt.show()

"""Menampilkan visualisasi 20 author teratas dengan jumlah buku terbanyak dalam dataset. Data dihitung berdasarkan frekuensi nama penulis, kemudian divisualisasikan dalam bentuk barplot horizontal. Terlihat dalam barplot tersebut author teratas adalah Agatha Christie, lalu William Shakespeare, disusul Stephen King dan seterusnya."""

# Konversi tahun jadi angka (untuk menghindari kesalahan parsing)
books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')

plt.figure(figsize=(14, 6))
sns.histplot(books['Year-Of-Publication'].dropna(), bins=50, kde=False, color='skyblue')
plt.title('Distribusi Tahun Publikasi Buku')
plt.xlabel('Tahun')
plt.ylabel('Jumlah Buku')
plt.tight_layout()
plt.show()

"""Menunjukkan visualisasi distribusi jumlah buku berdasarkan tahun publikasi. Data tahun terlebih dahulu dikonversi ke format numerik untuk menghindari error, lalu divisualisasikan menggunakan histogram. Terlihat bahwa sebagian besar buku dipublikasikan pada rentang tahun 1900 ke atas, dengan outlier di tahun-tahun ekstrem seperti 0 atau di luar rentang wajar. Sehingga, variabel ini mungkin tidak akan dipertimbangkan untuk digunakan dalam fitur pemodelan."""

top_publishers = books['Publisher'].value_counts().head(10)

plt.figure(figsize=(8, 8))
plt.pie(top_publishers.values, labels=top_publishers.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
plt.title('Distribusi 10 Publisher Teratas')
plt.axis('equal')
plt.tight_layout()
plt.show()

"""Visualisasi pie chart ini menunjukkan distribusi sepuluh penerbit buku teratas berdasarkan jumlah buku yang diterbitkan. Harlequin menjadi penerbit dengan kontribusi terbanyak sebesar 20.1%.

## Mengeksplorasi Data Rating
"""

ratings.info()

"""Menampilkan ringkasan informasi dari data rating pengguna terhadap buku pada dataframe "ratings" mulai dari kolom, jumlah baris, dan tipe data tiap kolomnya. Terlihat ada 3 kolom dengan jumlah baris 1.149.780 baris data dengan 2 kolom bertipe data numerik (int64), dan 1 kolom bertipe data object."""

print('Jumlah data user: ', len(ratings['User-ID'].unique()))
print('Jumlah data buku: ', len(ratings['ISBN'].unique()))
print('Jumlah data rating: ', len(ratings))

"""Menampilkan jumlah data unik tiap kolom, mulai dari user (User-ID), buku (ISBN), dan rating (Book-Rating)."""

pd.set_option('display.float_format', '{:.2f}'.format)
print(ratings.describe())

"""Menampilkan statistik deskriptif dari variabel numerik data rating, seperti nilai minimum, maksimum, rata-rata, dan kuartil yang digunakan untuk memahami sebaran nilai rating secara umum. Namun, pertama dilakukan pengaturan format tampilan angka desimal menjadi dua digit di belakang koma

## Mengeksplorasi Data User
"""

users.info()

"""Menampilkan ringkasan informasi dari data pengguna pada dataframe "users" mulai dari kolom, jumlah baris, dan tipe data tiap kolomnya. Terlihat ada 3 kolom dengan jumlah baris 278.858 baris data dengan 2 kolom bertipe data numerik (int64 dan float64), dan 1 kolom bertipe data object."""

users.head()

"""Menampilkan contoh data users yang terdiri dari id, lokasi pengguna, dan usia pengguna. Namun, fitur-fitur ini tidak kami gunakan dalam studi kasus sistem rekomendasi kali ini, karena yang akan digunakan adalah berdasarkan item (fitur buku) bukan user (demografi pengguna).

# Data Preparation

## Menggabungkan Seluruh Data
"""

all_books_rate = ratings
all_books_rate

"""Membuat dataframe "all_books_rate" yang merupakan salinan dari dataframe "ratings" dan menampilkan contoh isinya."""

all_books = pd.merge(all_books_rate, books[['ISBN','Book-Title','Book-Author', 'Publisher']], on='ISBN', how='left')
all_books

"""Melakukan penggabungan atau join antara dataframe "all_books_rate" yang dibuat sebelumnya dan informasi buku dari books berdasarkan kolom ISBN menggunakan metode left join. Hasilnya adalah DataFrame all_books yang berisi rating buku lengkap dengan judul, penulis, dan penerbitnya.

## Menangani Missing Value
"""

# Mengecek missing value pada dataframe all_books
all_books.isnull().sum()

"""Melakukan pengecekan missing value pada DataFrame all_books yang merupakan hasil penggabungan sebelumnya antara data rating dengan informasi detail buku seperti judul, penulis, dan penerbit. Terlihat ada beberapa missing value di kolom seperti judul, author, dan publisher.

Missing value yang muncul merupakan hal wajar karena proses penggabungan dilakukan dengan left join, sehingga tidak semua ISBN pada rating memiliki data lengkap di tabel buku. Oleh karena itu, baris yang memiliki missing value akan dibuang agar tidak memengaruhi hasil analisis dan sistem rekomendasi.
"""

# Membersihkan missing value dengan fungsi dropna()
all_books_clean = all_books.dropna()
all_books_clean

"""Melakukan drop atau membuang baris yang memiliki missing value dan menyimpannya pada dataframe "all_books_clean"."""

# Mengecek missing value pada dataframe all_books
all_books_clean.isnull().sum()

"""Melakukan pengecekan kembali hasil penghapusan baris dengan missing value tadi.

## Menangani Data Duplikat pada ISBN
"""

preparation = all_books_clean.drop_duplicates('ISBN')
preparation

"""Pada tahap ini, dilakukan penanganan duplikat pada kolom ISBN, karena pada pemodelan content-based filtering nanti diperlukan representasi unik untuk setiap buku. Duplikasi dapat menyebabkan bias dalam perhitungan kemiripan antar buku, sehingga perlu dihapus agar hasil rekomendasi lebih akurat. Hasilnya disimpan pada dataframe "preparation".

## Mengambil 10.000 data acak
"""

preparation = preparation.sample(frac=1, random_state=42).reset_index(drop=True)
preparation = preparation[:10000]

"""Pada tahap ini, data diambil sebanyak 10.000 saja secara acak menggunakan random_state untuk memastikan hasil yang konsisten. Hal ini dilakukan untuk mengurangi ukuran data karena keterbatasan sumber daya pengolahan data serta demi efisiensi proses pelatihan model tanpa mengorbankan keragaman sampel.

## Membuat Dictionary
"""

# Mengonversi data series ISBN menjadi dalam bentuk list
book_id = preparation['ISBN'].tolist()

# Mengonversi data series Book-Title menjadi dalam bentuk list
book_title = preparation['Book-Title'].tolist()

# Mengonversi data series Book-Author menjadi dalam bentuk list
book_author = preparation['Book-Author'].tolist()

# Mengonversi data series Publisher menjadi dalam bentuk list
book_publisher = preparation['Publisher'].tolist()

print(len(book_id))
print(len(book_title))
print(len(book_author))
print(len(book_publisher))

"""Pada tahap ini, data dari beberapa kolom seperti ISBN, judul buku, penulis, dan penerbit dikonversi menjadi bentuk list. Hal ini bertujuan untuk mempermudah proses pemetaan atau manipulasi data pada tahap selanjutnya."""

# Membuat dictionary untuk data "book_id", "book_title", "book_author", "book_publisher"
books_new = pd.DataFrame({
    'id' : book_id,
    'title' : book_title,
    'author' : book_author,
    'publisher' : book_publisher
})

books_new

"""Data yang sudah dikonversi ke dalam bentuk list sebelumnya kemudian disatukan kembali ke dalam sebuah dictionary yaitu DataFrame baru bernama books_new. DataFrame ini memuat informasi utama buku seperti ID, judul, penulis, dan penerbit yang siap digunakan untuk pemodelan atau analisis selanjutnya.

# Model Development Content-Based Filtering
"""

data = books_new
data.sample(5)

"""Menyalin data yang ada pada dataframe "books_new" ke dalam dataframe salinan baru, yaitu "data" dan menampilkan 5 sampel data tersebut."""

data['combined'] = (
    data['title'].fillna('') + ' ' +
    data['author'].fillna('') + ' ' +
    data['publisher'].fillna('')
)

"""Pada tahap ini, dilakukan penggabungan tiga kolom penting dalam data—yaitu title, author, dan publisher—menjadi satu kolom baru bernama combined. Tujuan dari langkah ini adalah untuk membuat representasi teks gabungan dari informasi buku, yang nantinya bisa digunakan untuk pemodelan berbasis konten (content-based filtering), seperti TF-IDF dan cosine similarity."""

def clean_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^\w\s]', '', text)
        text = text.lower()
        return text
    return ""

data['cleaned'] = data['combined'].apply(clean_text)

"""Pada tahap ini, dilakukan pembersihan teks dengan menghapus tanda baca dan mengubah semua huruf menjadi huruf kecil serta mengembalikan string kosong jika input bukan tipe string (menghindari error). Proses ini diterapkan pada kolom combined dan hasilnya disimpan di kolom cleaned. Tujuannya adalah untuk menyiapkan data teks yang konsisten sebelum dilakukan proses vektorisasi."""

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(data['cleaned'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Pada tahap ini, dilakukan proses vektorisasi terhadap teks yang sudah dibersihkan menggunakan TF-IDF untuk mengubah data teks menjadi representasi numerik. Stop words dalam bahasa Inggris dihapus agar hasil lebih relevan. Selanjutnya, ditampilkan ukuran matriks TF-IDF yang terbentuk, yang merepresentasikan jumlah data dan kata unik dari seluruh data."""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Pada tahap ini, dilakukan perhitungan cosine similarity terhadap matriks TF-IDF untuk mengukur tingkat kemiripan antar buku berdasarkan fitur teks gabungan (judul, penulis, dan penerbit). Hasilnya berupa matriks kesamaan yang menunjukkan seberapa mirip setiap buku dengan buku lainnya."""

# Membuat DataFrame similarity dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat sebagian dari similarity matrix antar buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Pada tahap ini, matriks cosine similarity dikonversi menjadi sebuah DataFrame dengan baris dan kolom berupa judul buku agar lebih mudah dianalisis. Kemudian, ditampilkan sebagian isi matriks tersebut secara acak untuk melihat nilai kemiripan antar judul buku dalam format yang terbaca."""

def book_recommendations(judul_buku, similarity_data=cosine_sim_df, items=data[['title', 'author', 'publisher']], k=10):
    """
    Memberikan rekomendasi buku berdasarkan kemiripan konten (TF-IDF cosine similarity).
    """
    try:
        # Pastikan judul buku ada dan hanya satu
        if judul_buku not in similarity_data.columns:
            return f"Judul '{judul_buku}' tidak ditemukan dalam similarity data."

        # Ambil kolom sebagai Series, bukan DataFrame
        sim_scores = similarity_data.loc[:, judul_buku]

        # Pastikan Series, bukan DataFrame
        if isinstance(sim_scores, pd.DataFrame):
            sim_scores = sim_scores.iloc[:, 0]

        # Urutkan nilai similarity
        sim_scores = sim_scores.sort_values(ascending=False)

        # Buang dirinya sendiri
        sim_scores = sim_scores.drop(labels=[judul_buku], errors='ignore')

        # Ambil top-k index
        top_titles = sim_scores.head(k).index

        # Gabungkan dengan metadata
        return items[items['title'].isin(top_titles)].drop_duplicates('title').head(k)

    except Exception as e:
        return f"Terjadi error: {e}"

"""Membuat fungsi book_recommendations(). Fungsi book_recommendations() digunakan untuk memberikan rekomendasi buku berdasarkan kemiripan konten menggunakan metode TF-IDF dan cosine similarity. Fungsi ini mencari buku-buku yang paling mirip dengan judul yang diberikan, lalu mengurutkannya berdasarkan skor kemiripan tertinggi dan menampilkan sejumlah buku teratas yang direkomendasikan. Rekomendasi yang dihasilkan berupa informasi judul, penulis, dan penerbit dari buku-buku yang memiliki konten paling mirip."""

data[data.title.eq("A Portrait of the Artist As a Young Man")]

"""Menampilkan baris data yang memiliki judul buku "A Portrait of the Artist As a Young Man" dalam kolom title."""

book_recommendations("A Portrait of the Artist As a Young Man", k=10)

"""Memanggil fungsi book_recommendations untuk memberikan 5 rekomendasi buku yang mirip dengan "A Portrait of the Artist As a Young Man" berdasarkan kemiripan konten menggunakan cosine similarity dari TF-IDF.Output diatas merupakan hasil rekomendasi 5 buku yang mirip dengan buku "A Portrait of the Artist As a Young Man".

## 📊 Evaluasi Content-Based Filtering dengan Recall@10

Sistem rekomendasi dijalankan untuk buku **"A Portrait of the Artist As a Young Man"** dengan `k=10`, yang berarti menampilkan 10 rekomendasi teratas berdasarkan kemiripan konten (judul, penulis, penerbit).

Berikut adalah hasil rekomendasinya:

| No | Judul Buku                                              | Author              | Publisher           | Relevan? |
|----|----------------------------------------------------------|---------------------|---------------------|----------|
| 1  | A Man of Affairs (Signet Regency Romance)                | Anne Barbour        | Signet Book         | ✅       |
| 2  | The Nobody (Signet Regency Romance)                      | Diane Farr          | Signet Book         | ✅       |
| 3  | George Bellows: American Artist (Writers on Art)         | Joyce Carol Oates   | Harpercollins       | ❌       |
| 4  | The Runaways (Signet Regency Romance)                    | Barbara Hazard      | Signet Book         | ✅       |
| 5  | Dubliners                                                | James Joyce         | Signet Classics     | ✅       |
| 6  | Dubliners (Essential.penguin S.)                         | James Joyce         | Penguin Books Ltd   | ✅       |
| 7  | A London Season (Signet Regency Romance)                 | Joan Wolf           | Signet Book         | ✅       |
| 8  | Impostress (Signet Historical Romance)                   | Lisa Jackson        | Signet Book         | ✅       |
| 9  | Arabia-Una nubecilla-Duplicados                          | James Joyce         | Sudamericana        | ✅       |
|10  | Gentleman Jack (Signet Regency Romance)                  | Margaret Summerville| Signet Book         | ✅       |

**Kriteria relevansi**:
- **Penulis**: Sama (James Joyce)
- **Penerbit**: Sama atau sangat mirip (misalnya "Signet Book")

### ✅ Perhitungan Recall@10

Terdapat **10 rekomendasi** yang diberikan (`k=10`), dan dari daftar tersebut, **9 buku** dianggap **relevan** karena memiliki kemiripan penerbit atau penulis dengan buku sumber.

$$
Recall@10 = \frac{9}{10} = 0.90
$$


Dengan nilai **Recall@10 = 0.90**, dapat disimpulkan bahwa sistem mampu merekomendasikan buku-buku yang relevan dengan baik dalam 10 besar hasil.

---

# Model Development Collaborative Filtering

## Menyalin data rating dan mengambil 10.000 sampel data secara acak
"""

# Ambil dataframe rating buku
df = ratings.copy()

# Mengacak dan mengambil 10.000 data saja
df = df.sample(frac=1, random_state=42).head(10000)
df

"""Membuat salinan dari DataFrame ratings ke dalam DataFrame baru df. Selanjutnya, data diacak dan diambil sebanyak 10.000 baris secara acak untuk mengurangi ukuran data, demi efisiensi proses pelatihan model dan keterbatasan sumber daya komputasi.

## Melakukan encoding fitur
"""

# Mengubah User-ID menjadi list unik
user_ids = df['User-ID'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah ISBN menjadi list unik
book_ids = df['ISBN'].unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

"""Pada tahap ini, dilakukan encoding terhadap kolom User-ID dan ISBN menjadi angka unik agar dapat digunakan dalam pelatihan model. Dictionary user_to_user_encoded dan book_to_book_encoded digunakan untuk mengubah ID asli ke bentuk numerik, sedangkan user_encoded_to_user dan book_encoded_to_book untuk mengubahnya kembali ke ID asli. Proses ini penting agar data dapat dimasukkan ke dalam model rekomendasi berbasis embedding."""

# Mapping ke dataframe
df['user'] = df['User-ID'].map(user_to_user_encoded)
df['book'] = df['ISBN'].map(book_to_book_encoded)

"""Pada tahap ini, dataframe df dimapping menggunakan dictionary encoding yang telah dibuat sebelumnya. Kolom User-ID diubah menjadi kolom user, dan ISBN menjadi kolom book, yang masing-masing berisi representasi numerik dari ID asli. Proses ini merupakan lanjutan dari tahap sebelumnya untuk mempersiapkan data dalam format numerik yang dibutuhkan oleh model rekomendasi berbasis embedding TensorFlow."""

# Convert rating ke float
df['rating'] = df['Book-Rating'].astype(np.float32)

# Info penting
num_users = len(user_ids)
num_books = len(book_ids)
min_rating = df['rating'].min()
max_rating = df['rating'].max()

print(f"Jumlah User: {num_users}, Jumlah Buku: {num_books}, Min Rating: {min_rating}, Max Rating: {max_rating}")

"""Selanjutnya, nilai rating pada kolom Book-Rating dikonversi ke tipe data float32 agar sesuai dengan kebutuhan pemrosesan model. Setelah itu, jumlah user unik, jumlah buku unik, serta nilai minimum dan maksimum rating yang tersedia dalam data ditampilkan untuk informasi."""

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Pada tahap ini, dibuat variabel x yang merepresentasikan pasangan user dan buku sebagai input fitur, serta y sebagai nilai rating yang telah dinormalisasi ke rentang 0–1. Selanjutnya, data dibagi menjadi dua bagian, yaitu 80% untuk pelatihan (train) dan 20% untuk validasi (val), yang nantinya digunakan untuk keperluan evaluasi performa model."""

class BookRecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(BookRecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size

    # Embedding untuk user
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)

    # Embedding untuk buku
    self.book_embedding = layers.Embedding(
        num_books,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_books, 1)

  def call(self, inputs):
    # Ambil embedding dan bias user
    user_vector = self.user_embedding(inputs[:, 0])
    user_bias = self.user_bias(inputs[:, 0])

    # Ambil embedding dan bias buku
    book_vector = self.book_embedding(inputs[:, 1])
    book_bias = self.book_bias(inputs[:, 1])

    # Dot product
    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    # Jumlahkan semua komponen
    x = dot_user_book + user_bias + book_bias

    # Output: Skor prediksi rating (0–1 karena sigmoid)
    return tf.nn.sigmoid(x)

"""Pada tahap ini, didefinisikan sebuah class model rekomendasi berbasis neural collaborative filtering bernama BookRecommenderNet. Model ini menggunakan layer embedding untuk memetakan user dan buku ke dalam representasi vektor berdimensi tertentu, serta menambahkan bias masing-masing. Kemudian, dilakukan dot product antara embedding user dan buku, ditambah bias, dan hasil akhirnya diproses melalui fungsi aktivasi sigmoid untuk menghasilkan skor prediksi rating dalam rentang 0–1."""

model = BookRecommenderNet(num_users, num_books, embedding_size=50)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Pada tahap ini, dibuat objek model BookRecommenderNet dengan ukuran embedding sebesar 50. Model kemudian dikompilasi menggunakan binary crossentropy sebagai fungsi loss karena target rating sudah dinormalisasi ke rentang 0–1, serta optimizer Adam dengan learning rate 0.001. Selain itu, ditambahkan metrik evaluasi Root Mean Squared Error (RMSE) untuk memantau performa model selama pelatihan."""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Selanjutnya, dilakukan visualisasi performa model selama proses pelatihan menggunakan metrik Root Mean Squared Error (RMSE). Grafik menampilkan perbandingan nilai RMSE antara data latih dan data validasi di setiap epoch. Tujuannya adalah untuk melihat bagaimana error model berubah seiring waktu, serta untuk mengidentifikasi apakah model mengalami overfitting atau tidak."""

# Gunakan df yang sudah disampling 10.000 data
book_df = books_new

# Ambil 1 user secara acak dari df 10rb
user_id = df['User-ID'].sample(1).iloc[0]

# Buku-buku yang sudah dibaca user ini (dari 10rb data saja)
books_read_by_user = df[df['User-ID'] == user_id]

# ISBN buku yang belum dibaca user
books_not_read = book_df[~book_df['id'].isin(books_read_by_user.ISBN.values)]['id']

# Ambil hanya buku yang sudah ada di book_to_book_encoded (yaitu dari 10rb data)
books_not_read = list(
    set(books_not_read)
    .intersection(set(book_to_book_encoded.keys()))
)

# Ubah ke format list of list [ [encoded_book_id], ... ]
books_not_read = [[book_to_book_encoded.get(x)] for x in books_not_read]

# Encode user ID
user_encoder = user_to_user_encoded.get(user_id)

# Bentuk array [ [user_id, book_id], ... ]
user_book_array = np.hstack(
    ([[user_encoder]] * len(books_not_read), books_not_read)
)

"""Sistem menyiapkan data untuk proses prediksi rekomendasi dengan memilih satu pengguna secara acak dan mengidentifikasi buku-buku yang belum pernah dibaca berdasarkan sampel 10.000 data. Buku-buku tersebut kemudian disaring agar hanya mencakup yang telah dikenali oleh model melalui proses encoding. Setelah itu, ID pengguna dan ID buku-buku yang belum dibaca dikonversi ke dalam format array dua dimensi sebagai input bagi model untuk memprediksi rating."""

# Prediksi rating user terhadap buku-buku yang belum dibaca
ratings = model.predict(user_book_array).flatten()

# Ambil 10 prediksi rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Ambil ISBN buku yang direkomendasikan (decode dari index)
recommended_book_ids = [
    book_encoded_to_book.get(books_not_read[x][0]) for x in top_ratings_indices
]

# Tampilkan hasil rekomendasi
print('Showing recommendations for user:', user_id)
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)

# Ambil buku dengan rating tertinggi dari user
top_books_user = (
    books_read_by_user.sort_values(
        by='rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

# Tampilkan informasi buku favorit user
book_df_rows = book_df[book_df['id'].isin(top_books_user)]
for row in book_df_rows.itertuples():
    print(f"{row.title} - {row.author} ({row.publisher})")

print('----' * 8)
print('Top 10 Book Recommendations')
print('----' * 8)

# Ambil detail buku dari rekomendasi
recommended_books = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(f"{row.title} - {row.author} ({row.publisher})")

"""Setelah data pengguna dan daftar buku yang belum dibaca disiapkan, model yang telah dilatih digunakan untuk memprediksi rating terhadap buku-buku tersebut. Dari hasil prediksi, sistem memilih 10 buku dengan skor tertinggi sebagai rekomendasi. Untuk melengkapi hasilnya, ditampilkan juga daftar buku favorit pengguna berdasarkan rating tertinggi yang pernah ia berikan, disusul dengan 10 rekomendasi buku yang kemungkinan besar akan disukai."""